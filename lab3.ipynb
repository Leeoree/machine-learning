{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb158fc-cbf3-4b2f-9252-fcdea12f51db",
   "metadata": {},
   "source": [
    "# Лабораторная работа №3. Линейные модели для классификации\n",
    "\n",
    "В задачах классификации, в отличие от задач регрессии, требуется по образу объекта определить его принадлежность к тому или иному классу.\n",
    "В данной работе будет рассмотрена только задача бинарной (двухклассовой) классификации.\n",
    "В качестве ответа классификатора используется\n",
    "$$\n",
    "P(C_1 | \\mathbf\\phi) = \\sigma( \\mathbf{w}^T \\mathbf\\phi),\n",
    "$$\n",
    "где\n",
    "$$\n",
    "\\sigma(a) = \\frac{1}{1 + \\mathrm{e}^{-a}}.\n",
    "$$\n",
    " \n",
    "В случае логистической регрессии минимизируется функция штрафа\n",
    "$$\n",
    "E(\\mathbf{w}) = -\\sum_{n=1}^N\\left( t_n \\ln y_n + (1 - t_n) \\ln (1 - y_n) \\right).\n",
    "$$\n",
    "\n",
    "Её градиент равен\n",
    "$$\n",
    "\\nabla E(\\mathbf{w}) = \\sum_{n=1}^N (y_n - t_n) \\mathbf\\phi(\\mathbf{x}_n) =\n",
    "\\sum_{n=1}^N \\left( \\sigma(\\mathbf{w}^T\\mathbf\\phi_n - t_n \\right) \\mathbf\\phi_n = \\mathbf\\Phi^T (\\mathbf{y} - \\mathbf{T}),\n",
    "$$\n",
    "где $\\mathbf{y} = (y_1, \\dots, y_N)^T$ $-$ отклики распознавателя, $\\mathbf{T} = (t_1, \\dots, t_N)^T$ $-$ метки классов.\n",
    " \n",
    "Выпишем классический метод градиентного спуска в матричной форме:\n",
    "$$\n",
    "\\mathbf{w}_{i+1} = \\mathbf{w}_i - \\alpha\\nabla E,\\;\\;\\; i = 1, 2, \\dots,\n",
    "$$\n",
    "где $\\mathbf{w}_0$ задано, а $\\nabla E = \\nabla E(\\mathbf{w}_i, T)$.\n",
    "Заметим, что здесь $T = \\left\\{ (\\mathbf{x}_k, t_k) \\right\\}_{k=1}^N$.\n",
    "То есть в итерационной процедуре на каждой итерации для расчета $\\nabla{E}$ используются вся выборка целиком.\n",
    "\n",
    "Будем теперь вместо полной выборки $T = \\left\\{ (\\mathbf{x}_k, t_k) \\right\\}_{k=1}^N$ на каждой итерации случайно выбирать $K$ элементов из T:\n",
    "$T_i = \\left( (\\mathbf{x}_{i_1}, t_{i_1}), (\\mathbf{x}_{i_2}, t_{i_2}), \\dots, (\\mathbf{x}_{i_K}, t_{i_K}) \\right)$.\n",
    "Назовем $T_i$ пакетом или пачкой (batch).\n",
    "Индекс $i$ отражает то, что пакет векторов из обучающей выборки выбирается отдельно на каждой итерации.\n",
    "\n",
    "При $K = 1$ получаем так называемый _стохастический градиентный спуск_ (stochastic gradient descent, SGD).\n",
    "\n",
    "При $K = N$ используется вся выборка, получаем _классический метод градиентного спуска_ (vanilla gradient descent).\n",
    "\n",
    "В остальных случаях ($1 < K < N$) метод называют _mini-batch gradient descent_.\n",
    "\n",
    "На практике обычно _mini-batch gradient descent_ сходится быстрее, чем _vanilla gradient descent_.\n",
    "\n",
    "Вместе со стохастическим градиентным спуском часто применяют момент.\n",
    "Момент не позволяет резко измениться направлению спуска.\n",
    "Это позволяет достичь более плавного и быстрого спуска.\n",
    "В ряде случаев использование момента также позволяет не «свалиться» в локальный минимум.\n",
    "\n",
    "**Momentum**.\n",
    "Итерационный поиск осуществляется по следующему алгоритму:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\mathbf{w}_{i+1} = \\mathbf{w}_i + \\mathbf{v}_i,\\;\\;\\; i = 1, 2, \\dots, \\\\\n",
    "    \\mathbf{v}_i = \\gamma \\mathbf{v}_{i-1} - \\alpha \\nabla{E} \\left( \\mathbf{w}_i, (\\mathbf{x}_{k_i}, t_{k_i} \\right), \\\\\n",
    "    \\mathbf{v}_0 = 0.\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Для параметра $\\gamma$ обычно выбирается значение 0.9 или близкое к этому."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f09fa2-670d-4673-8de6-2f97f472c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import datasets, model_selection, metrics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "mpl.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35852c-eeee-4b57-9d0d-2426981a5766",
   "metadata": {},
   "source": [
    "## Задание №1\n",
    "\n",
    "Разделите выборку на обучающую и тестовую.\n",
    "Размер тестовой выборки - 20% от размера исходной выборки.\n",
    "Параметр `n_features` определяется как `3 + mod(<порядковый номер в списке>, 3)`, `n_samples` необходимо взять `600 - 100 * mod(<порядковый номер в списке>, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64762170-379f-4b96-a3db-2ab405870395",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sk.datasets.make_classification(n_samples=???, n_features=???, random_state=145)\n",
    "x_train, x_test, y_train, y_test = ???\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a828b-4c99-46a6-bdad-c310461f97bb",
   "metadata": {},
   "source": [
    "## Задание №2\n",
    "\n",
    "Реализуйте алгоритм _mini-batch gradient descent_.\n",
    "В качестве критерия выхода возьмите ограничение по количеству эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263a23b-1805-4ab7-b667-f4dd0d5ec14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, batch_size, alpha=1e-3, eps=1e-6, epoch=100):\n",
    "        self.rng = np.random.default_rng(seed=489);\n",
    "        self.alpha, self.eps, ... = ???\n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y):\n",
    "        self.w = self.rng.???\n",
    "        self.loss, self.precision = [], []\n",
    "        self.val_loss, self.val_precision = [], []\n",
    "\n",
    "        xx = ??? np.concatenate\n",
    "        idxs = range(0, xx.shape[0], self.batch_size)\n",
    "        for i in range(???):\n",
    "            ???\n",
    "            for idx in ??? rng.permutation\n",
    "                N = ???\n",
    "                deriv = ??? np.asmatrix\n",
    "                self._update(xx???, deriv)\n",
    "\n",
    "            self.loss.append(sk.metrics.???)\n",
    "            self.precision.append(sk.metrics.???)\n",
    "            self.val_loss.append(sk.metrics.???)\n",
    "            self.val_precision.append(sk.metrics.???)\n",
    "   \n",
    "    def _update(self, x, derr):\n",
    "        self.w = ???\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        xx = ???\n",
    "        return ??? xx self.w reshape(-1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.predict_proba(x) ???\n",
    "\n",
    "# Тестирование\n",
    "\n",
    "test_x = np.array([[1, 1, 1], [2, 4, 16], [3, 9, 27]])\n",
    "gd = GradientDescent(batch_size=10)\n",
    "gd.fit(test_x, [1, 0, 1], val_x=test_x, val_y=[1, 0, 1])\n",
    "print(\"Response\", gd(test_x))\n",
    "assert sk.metrics.precision_score(gd(test_x), [1, 0, 1]) == 1.0, 'Метод недостаточно точный'\n",
    "assert sk.metrics.accuracy_score(gd(test_x), [1, 0, 1]) > 0.6, 'Метод недостаточно точный'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fabce0-0bdb-4f9c-9f09-38907ddb9829",
   "metadata": {},
   "source": [
    "## Задание №3\n",
    "\n",
    "Протестируйте написанный алгоритм, подберите параметры градиентного спуска, дающие после обучения классификатор с наилучшей точностью\n",
    "($\\text{точность} = \\frac{\\text{количество правильно классифицированных векторов}}{\\text{количество векторов}}$).\n",
    "Сравните классический градиентный спуск, стохастический и mini-batch.\n",
    "Для начала возьмите следующие параметры:\n",
    "\n",
    "* случайные начальные значения весов (равномерное распределение на отрезке $[-1; 1]$ для каждой компоненты);\n",
    "* размер шага обучения $\\alpha = 10^{-5}$;\n",
    "* размер пакета _batch size_ = 20;\n",
    "* максимальное количество эпох 1000.\n",
    "\n",
    "Приведите в отчет полученную точность и значение функции штрафа на тестовой и обучающей выборках.\n",
    "Постройте графики для каждого тестируемого размера батча:\n",
    "\n",
    "* зависимость функции штрафа на обучающей и тестовой выборках от номера эпохи;\n",
    "* зависимость точности на обучающей и тестовой выборках от номера эпохи;\n",
    "* зависимость времени выполнения алгоритма от размера батча."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8bfee-e3d8-4b87-be3f-069af0e69562",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "times = {}\n",
    "for batch in [1, 20, 50, 100, x_train.shape[0]]:\n",
    "    gd = GradientDescent(batch, epoch=???, alpha=???)\n",
    "    t = timeit.default_timer()\n",
    "    gd.fit(x_train, y_train, val_x=x_test, val_y=y_test)\n",
    "    times[batch] = timeit.default_timer() - t\n",
    "    y_pred = gd(x_test)\n",
    "    print(f\"Batch size = {batch}, precision = {sk.metrics.???(y_test, y_pred)}\")\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(2*6, 4))\n",
    "    ax0.plot(range(len(gd.loss)), gd.loss, label='обучающая')\n",
    "    ax0.plot(range(len(gd.val_loss)), gd.???, label='тестовая')\n",
    "    ax0.set(xlabel='Номер эпохи', ylabel='Ошибка ???', title='Loss')\n",
    "    ax0.legend()\n",
    "    ax1.plot(range(len(gd.precision)), gd.precision, label='обучающая')\n",
    "    ???\n",
    "    ax1.set(???, title='Precision')\n",
    "    fig.suptitle(f\"Размер батча {batch}\")\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(times.keys(), times.values()) # python3.7+\n",
    "plt.xlabel(???)\n",
    "plt.ylabel(???)\n",
    "plt.title('Зависимость времени работы от размера батча')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b86a92-7f02-4363-aa91-9b9722b401af",
   "metadata": {},
   "source": [
    "**Вопросы**\n",
    "\n",
    "1. Зависит ли точность от размера батча? Почему?\n",
    "1. Зависит ли время работы алгоритма от размера батча? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825d5cf-75fb-405a-a56f-bb68d05cb19a",
   "metadata": {},
   "source": [
    "**Ответы**\n",
    "\n",
    "1. ???\n",
    "1. ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6acc61b-ed41-4687-8eec-ed08ee07c0c2",
   "metadata": {},
   "source": [
    "## Задание №4\n",
    "\n",
    "Реализуйте метод оптимизации градиентного спуска _momentum_.\n",
    "Примените наследование от `GradientDescent` и перегрузите метод `_update`, в котором происходят обновления весов модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e9eab-223c-468a-b589-48e1395a8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumGD(GradientDescent):\n",
    "    def __init__(self, gamma, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def fit(self, x, *args, **kwargs):\n",
    "        self.v = ???\n",
    "        super().fit(x, *args, **kwargs)\n",
    "    \n",
    "    def _update(self, x, err):\n",
    "        self.v = ???\n",
    "        self.w += ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728a5ed-fcfc-4479-a5f9-14a393b91173",
   "metadata": {},
   "source": [
    "## Задание №5\n",
    "\n",
    "Подберите параметры, дающие улучшение сходимости (в большей степени это относится к параметру $\\gamma$).  \n",
    "\n",
    "* Сравните скорость сходимости со скоростью в задании 2.\n",
    "Для этого постройте графики, на которых сравниваются скорости сходимости и качества модели для **обучающей** выборки.\n",
    "* Сравните достигнутую точность с точностью в задании 2.\n",
    "Удалось ли лучше обучить модель?\n",
    "Чтобы эксперимент был «честным» следует запускать все методы с одними и теми же параметрами и с одними и теми же начальными значениями весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7c0f9-a3a5-4002-b521-3c91aee320e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gd0 = GradientDescent(batch_size=50, epoch=1000, alpha=1e-5)\n",
    "gd0.fit(x_train, y_train, val_x=x_test, val_y=y_test)\n",
    "\n",
    "for gamma in [0.1, 0.3, 0.6, 0.9, 1.0]:\n",
    "    gd = MomentumGD(gamma=gamma, ???)\n",
    "    gd.fit(???)\n",
    "    y_pred = gd(x_test)\n",
    "    print(f\"Gamma = {gamma}, precision = {sk.metrics.???(y_test, y_pred)}\")\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(2*6, 4))\n",
    "    ???\n",
    "    ax0.plot(??? label='обучающая')\n",
    "    ax0.plot(??? label='тестовая')\n",
    "    ax0.plot(??? label='обучающая GD')\n",
    "    ax0.set(xlabel='Номер эпохи', ylabel='Ошибка ???', title='Loss')\n",
    "    ax0.legend()\n",
    "    ???\n",
    "    fig.suptitle(f\"Коэффициент $\\gamma$ {gamma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16886334-347c-4d8a-8694-6be021ce1d4a",
   "metadata": {},
   "source": [
    "**Вопросы**\n",
    "\n",
    "1. Какой из алгоритмов быстрее достигает итоговой точности?\n",
    "1. С ростом коэффициента $\\gamma$ алгоритм сходится быстрее или медленее?\n",
    "1. Что происходит при очень большом коэффициенте $\\gamma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686202e-7aae-4328-9baa-8db9dda72f79",
   "metadata": {},
   "source": [
    "**Ответы**\n",
    "\n",
    "1. ???\n",
    "1. ???\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6de6e4-3462-457a-964e-b7cd716516e4",
   "metadata": {},
   "source": [
    "## Задание №6\n",
    "\n",
    "Для какой-либо из обученных моделей постройте график, на котором изображено разделение пространства признаков на две области.\n",
    "Проверьте, что разделяющая поверхность - линейная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff9cb3-adf0-432c-94a8-d3ea4b70edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = x_test[:, 0].min() - 1, x_test[:, 0].max() + 1\n",
    "y_min, y_max = x_test[:, 1].min() - 1, x_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "Z = gd0(np.c_[xx.ravel(), yy.ravel(), np.zeros((xx.ravel().shape[0], x_test.shape[1] - 2))])\n",
    "plt.contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "plt.scatter(x_test[:, 0], x_test[:, 1], c=y_test)\n",
    "plt.xlabel('X0')\n",
    "plt.ylabel('X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e5101-6111-43b2-943d-8e78530e7f81",
   "metadata": {},
   "source": [
    "## Задание №7\n",
    "\n",
    "Классы в выборке не являются линейно разделимыми.\n",
    "С данной проблемой можно бороться, воздействуя какой-то функцией на старые признаки:\n",
    "$$\n",
    "\\mathbf{\\phi}(\\mathbf{x}) = \\left( \\phi_1(x_1, \\dots, x_n), \\dots, \\phi_n(x_1, \\dots, x_n) \\right)^T.\n",
    "$$\n",
    "\n",
    "Покажите, как изменяется разделяющая поверхность, если задавать разную функцию отображения.\n",
    "Постройте график, на котором изображено разделение пространства признаков и приведите величину достигнутой точности на тестовой выборке.\n",
    "Попробуйте добиться лучшей точности (необязательно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacacc17-b3b6-48de-9fc6-989460d21983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "class MGD(GradientDescent):\n",
    "    def _func(self, x):\n",
    "        return ???\n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y):\n",
    "        super().fit(???)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return super().__call__(???)\n",
    "\n",
    "gdx = MGD(batch_size=50, epoch=1000, alpha=1e-5)\n",
    "gdx.fit(x_train, y_train, x_test, y_test)\n",
    "plt.plot(???)\n",
    "plt.plot(???)\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "Z = gdx(???)\n",
    "???\n",
    "\n",
    "print(\"Score =\", sk.metrics.???(y_test, gdx(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5f1c1-dd7c-44e5-9a5c-a0bc56a43dff",
   "metadata": {},
   "source": [
    "**Выводы**:\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
